### «Прогнозирование эвакуации пассажиров в конкурсе „Spaceship Titanic“ (Kaggle): задача бинарной классификации»

Почему именно эти предикторы и модели?
Предикторы (признаки):
Числовые (num_cols): возраст, суммарные траты, логарифмы трат — важны для предсказания, так как отражают поведение пассажиров.
Категориальные (cat_cols): планета отправления, пункт назначения, палуба и т. д. — влияют на вероятность целевого события.
Бинарные (bool_cols): признаки пропусков, наличие трат, взаимодействие признаков — помогают модели лучше понять структуру данных.

Модели:

LightGBM и XGBoost — лидеры в задачах классификации табличных данных, хорошо улавливают нелинейные зависимости.
Random Forest — более устойчивая к переобучению альтернатива бустингу, используется для диверсификации ансамбля.
Stacking — позволяет объединить сильные стороны разных моделей, часто даёт прирост качества.

Отличия моделей:
Бустинг (LightGBM, XGBoost) строит деревья последовательно, корректируя ошибки предыдущих. 
Хорошо работает с небольшими датасетами, чувствителен к выбросам.
Random Forest строит деревья параллельно, усредняет предсказания. 
Менее чувствителен к переобучению, но может уступать в точности бустингу.
Stacking использует предсказания базовых моделей как признаки для финальной модели, что позволяет комбинировать их сильные стороны.

Этот подход обеспечивает баланс между качеством, устойчивостью и разнообразием моделей, что важно для успешного участия 
в соревнованиях по машинному обучению
